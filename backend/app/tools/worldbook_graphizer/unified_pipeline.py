"""
ç»Ÿä¸€ä¸–ç•Œä¹¦æå–ç®¡çº¿

ä» SillyTavern V2 Lorebook JSON ä¸€æ­¥ç”Ÿæˆ WorldInitializer æ‰€éœ€çš„å…¨éƒ¨æ–‡ä»¶ï¼š
  maps.json, characters.json, world_map.json, character_profiles.json,
  world_graph.json, prefilled_graph.json, chapters_v2.json,
  monsters.json, items.json, skills.jsonï¼ˆ--enrich-entitiesï¼‰

æ‰€æœ‰ LLM æ­¥éª¤ï¼ˆå›¾è°±æå–ã€è¾¹é‡æ ‡æ³¨ã€ç« èŠ‚å¢å¼ºã€å®ä½“æå–ï¼‰å‡æ”¯æŒä¸¤ç§æ¨¡å¼ï¼š
- Batch APIï¼ˆé»˜è®¤ï¼‰ï¼š50% æˆæœ¬ä¼˜æƒ ï¼Œéœ€ç­‰å¾…æ’é˜Ÿï¼Œå…¨éƒ¨æ­¥éª¤èµ° Batch
- ç›´æ¥è°ƒç”¨ï¼ˆ--directï¼‰ï¼šå®æ—¶è¿”å›ï¼Œé€æ¡è°ƒç”¨ LLMï¼Œæ— æˆæœ¬ä¼˜æƒ 

æ³¨æ„ï¼šéƒ¨åˆ†æ¨¡å‹çš„ Batch API ä¸æ”¯æŒ thinking_config å­—æ®µï¼ˆå¦‚ gemini-3-pro-previewï¼‰ï¼Œ
æ­¤æ—¶éœ€åŠ  --thinking-level none æ¥ç¦ç”¨ thinkingï¼Œå¦åˆ™ Step 3b ä¼šæŠ¥ 400 é”™è¯¯ã€‚

ç”¨æ³•:
    # Batch API æ¨¡å¼ï¼ˆé»˜è®¤ï¼Œå«è¾¹é‡æ ‡æ³¨å’Œå®ä½“æå–ï¼‰
    python -m app.tools.init_world_cli extract \
        --input data/gs/worldbook.json \
        --output data/gs/structured/ \
        --model gemini-3-pro-preview \
        --thinking-level none \
        --relabel-edges --enrich-entities

    # ç›´æ¥è°ƒç”¨æ¨¡å¼
    python -m app.tools.init_world_cli extract \
        --input data/gs/worldbook.json \
        --output data/gs/structured/ \
        --direct --relabel-edges --enrich-entities
"""
from __future__ import annotations

import json
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional

import re

from google import genai
from google.genai import types

from app.config import settings
from .tavern_card_parser import TavernCardParser
from .map_extractor import MapExtractor
from .npc_classifier import NPCClassifier
from .graph_extractor import GraphExtractor
from .graph_prefill import GraphPrefiller
from .batch_helper import BatchRunner
from .models import (
    CharacterInfo, CharactersData, MapsData, NPCTier,
    WorldMap, WorldMapRegion,
)


class UnifiedWorldExtractor:
    """ç»Ÿä¸€ä¸–ç•Œä¹¦æå–ç¼–æ’å™¨"""

    def __init__(
        self,
        model: str = None,
        api_key: str = None,
        verbose: bool = True,
        thinking_level: str = "high",
    ):
        """
        Args:
            model: Gemini æ¨¡å‹åç§° (é»˜è®¤: gemini-3-flash-preview)
            api_key: API å¯†é’¥
            verbose: æ˜¯å¦è¾“å‡ºè¯¦ç»†ä¿¡æ¯
            thinking_level: æ€è€ƒçº§åˆ« (lowest/low/medium/high)ï¼Œç”¨äº Batch API æå–
        """
        self.model = model or settings.gemini_flash_model
        self.api_key = api_key
        self.verbose = verbose
        self.thinking_level = thinking_level

        self.parser = TavernCardParser()
        self.map_extractor = MapExtractor(model=self.model, api_key=self.api_key)
        self.npc_classifier = NPCClassifier(model=self.model, api_key=self.api_key)
        self.graph_extractor = GraphExtractor(
            model=self.model,
            api_key=self.api_key,
            verbose=self.verbose,
            thinking_level=self.thinking_level,
        )
        self.batch_runner = BatchRunner(
            model=self.model,
            api_key=self.api_key,
            verbose=self.verbose,
            log_fn=self._log,
        )

    def _log(self, msg: str) -> None:
        if self.verbose:
            print(msg)

    async def extract(
        self,
        lorebook_path: Path,
        output_dir: Path,
        mainlines_path: Optional[Path] = None,
        validate: bool = True,
        use_direct: bool = False,
        relabel_edges: bool = False,
        enrich_entities: bool = False,
    ) -> Dict[str, Any]:
        """
        æ‰§è¡Œç»Ÿä¸€æå–ç®¡çº¿

        Args:
            lorebook_path: SillyTavern V2 Lorebook JSON è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•
            mainlines_path: å¯é€‰çš„ mainlines.json è·¯å¾„
            validate: æ˜¯å¦éªŒè¯ä¸­é—´ç»“æœ
            use_direct: ä½¿ç”¨ç›´æ¥ LLM è°ƒç”¨è€Œé Batch API
            relabel_edges: æ˜¯å¦é‡æ ‡æ³¨ unknown è¾¹ç±»å‹
            enrich_entities: æ˜¯å¦æå– D&D å®ä½“æ•°æ®

        Returns:
            åŒ…å«ç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
        """
        output_dir.mkdir(parents=True, exist_ok=True)
        stats: Dict[str, Any] = {"start_time": datetime.now().isoformat()}

        # â”€â”€ Step 1: è§£æé…’é¦†å¡ç‰‡ï¼ˆç”¨äºåœ°å›¾/è§’è‰²æå–çš„ markdownï¼‰ â”€â”€
        self._log("[Step 1] Parsing lorebook JSON...")
        data = self.parser.parse(lorebook_path)
        if self.verbose:
            self.parser.print_summary(data)

        graphable_entries = self.parser.get_graphable_entries(data)
        self._log(f"  Graphable entries: {len(graphable_entries)}")
        stats["total_entries"] = len(data.entries)
        stats["graphable_entries"] = len(graphable_entries)

        # â”€â”€ Step 2: æ ¼å¼åŒ–å…¨æ–‡ markdownï¼ˆåœ°å›¾å’Œè§’è‰²æå–ç”¨ï¼‰ â”€â”€
        self._log("\n[Step 2] Formatting worldbook markdown...")
        worldbook_md = self.graph_extractor._format_worldbook_markdown(
            graphable_entries, max_content_chars=8000
        )
        self._log(f"  Markdown size: {len(worldbook_md):,} chars")

        # â”€â”€ Step 3a: æå–åœ°å›¾ â”€â”€
        self._log("\n[Step 3a] Extracting maps...")
        maps_data = await self.map_extractor.extract(worldbook_md)
        self._log(f"  Found {len(maps_data.maps)} maps")

        if validate:
            map_errors = self.map_extractor.validate(maps_data)
            if map_errors:
                self._log(f"  Map warnings: {len(map_errors)}")
                for err in map_errors[:5]:
                    self._log(f"    - {err}")

        # â”€â”€ Step 3b: æå–çŸ¥è¯†å›¾è°± â”€â”€
        if use_direct:
            self._log(f"\n[Step 3b] Extracting world graph (direct, thinking={self.thinking_level})...")
            self._log(f"  Model: {self.model}")
            graph_data = await self.graph_extractor.extract_direct(
                worldbook_md=worldbook_md,
                entries=graphable_entries,
            )
        else:
            self._log(f"\n[Step 3b] Extracting world graph (Batch API, thinking={self.thinking_level})...")
            self._log(f"  Model: {self.model}")
            batch_temp_dir = output_dir / "batch_temp"
            graph_data = await self.graph_extractor.build_graph(
                json_path=lorebook_path,
                output_dir=batch_temp_dir,
            )
        self._log(f"  Nodes: {len(graph_data.nodes)}, Edges: {len(graph_data.edges)}")
        stats["world_graph_nodes"] = len(graph_data.nodes)
        stats["world_graph_edges"] = len(graph_data.edges)

        # â”€â”€ Step 4: NPC åˆ†ç±»ï¼ˆä¾èµ– mapsï¼‰ â”€â”€
        self._log("\n[Step 4] Classifying NPCs...")
        characters_data = await self.npc_classifier.classify(worldbook_md, maps_data)
        main_count = sum(1 for c in characters_data.characters if c.tier.value == "main")
        secondary_count = sum(1 for c in characters_data.characters if c.tier.value == "secondary")
        passerby_count = sum(1 for c in characters_data.characters if c.tier.value == "passerby")
        self._log(f"  Characters: {len(characters_data.characters)}")
        self._log(f"    main={main_count}, secondary={secondary_count}, passerby={passerby_count}")
        stats["characters"] = len(characters_data.characters)

        if validate:
            char_errors = self.npc_classifier.validate(characters_data, maps_data)
            if char_errors:
                self._log(f"  Character warnings: {len(char_errors)}")
                for err in char_errors[:5]:
                    self._log(f"    - {err}")

        # â”€â”€ Step 4b: å°† world_graph ä¸­æœªè¢« NPC åˆ†ç±»å™¨è¦†ç›–çš„è§’è‰²å›å¡« â”€â”€
        self._log("\n[Step 4b] Reconciling characters from world graph...")
        characters_data = self._reconcile_characters(characters_data, graph_data, maps_data)
        # æ›´æ–°ç»Ÿè®¡
        main_count = sum(1 for c in characters_data.characters if c.tier.value == "main")
        secondary_count = sum(1 for c in characters_data.characters if c.tier.value == "secondary")
        passerby_count = sum(1 for c in characters_data.characters if c.tier.value == "passerby")
        self._log(f"  Characters after reconciliation: {len(characters_data.characters)}")
        self._log(f"    main={main_count}, secondary={secondary_count}, passerby={passerby_count}")
        stats["characters_after_reconcile"] = len(characters_data.characters)

        # â”€â”€ Step 5: ç”Ÿæˆ world_map.jsonï¼ˆçº¯è§„åˆ™é€»è¾‘ï¼‰ â”€â”€
        self._log("\n[Step 5] Generating world map...")
        world_map = generate_world_map(maps_data)
        self._log(f"  Regions: {len(world_map.regions)}")

        # â”€â”€ Step 6: ç”Ÿæˆ character_profiles.json â”€â”€
        self._log("\n[Step 6] Generating character profiles...")
        profiles = self.npc_classifier.to_character_profiles(characters_data)
        self._log(f"  Profiles: {len(profiles)}")

        # â”€â”€ Step 7: ä¿å­˜å…¨éƒ¨ä¸­é—´æ–‡ä»¶ â”€â”€
        self._log(f"\n[Step 7] Saving files to {output_dir}...")
        _save_json(output_dir / "maps.json", maps_data.model_dump())
        _save_json(output_dir / "characters.json", characters_data.model_dump())
        _save_json(output_dir / "world_map.json", world_map.model_dump())
        _save_json(output_dir / "character_profiles.json", profiles)

        # ä¿å­˜ world_graph.jsonï¼ˆGraphPrefiller æ¶ˆè´¹ï¼‰
        def _serialize(obj):
            if isinstance(obj, datetime):
                return obj.isoformat()
            raise TypeError(f"Not serializable: {type(obj)}")

        _save_json(
            output_dir / "world_graph.json",
            graph_data.model_dump(),
            default=_serialize,
        )

        # ä¿å­˜/ç”Ÿæˆ mainlines.json
        if mainlines_path and mainlines_path.exists():
            mainlines_data = json.loads(mainlines_path.read_text(encoding="utf-8"))
            # å…¼å®¹æ—§ mainlines.jsonï¼šç¼ºå°‘ v2 ç¼–æ’å­—æ®µæ—¶è‡ªåŠ¨è¡¥é½ï¼ˆPhase 3ï¼‰
            if self._needs_chapter_orchestration(mainlines_data):
                self._log("\n[Step 7b] Enriching existing mainlines with chapter orchestration...")
                chapters_existing = mainlines_data.get("chapters", [])
                mainlines_existing = mainlines_data.get("mainlines", [])
                volumes: Dict[str, Dict[str, Any]] = {}
                if isinstance(mainlines_existing, list):
                    for mainline in mainlines_existing:
                        if not isinstance(mainline, dict):
                            continue
                        mainline_id = str(mainline.get("id", "")).strip()
                        if mainline_id:
                            volumes[mainline_id] = mainline

                try:
                    if isinstance(chapters_existing, list) and chapters_existing:
                        mainlines_data["chapters"] = await self._extract_chapter_orchestration(
                            chapters_existing,
                            volumes,
                            maps_data,
                            chars_data=characters_data,
                            output_dir=output_dir,
                            use_direct=use_direct,
                        )
                except Exception as exc:
                    self._log(f"  Warning: Existing mainlines orchestration enrichment failed: {exc}")

            # å…œåº•è¡¥é½ v2 å¿…è¦å­—æ®µï¼ˆç”¨äº legacy æ•°æ®è¿ç§»åˆ° strict-v2ï¼‰
            self._ensure_v2_story_defaults(mainlines_data)

            if settings.narrative_v2_strict_mode:
                self._validate_mainlines_v2(mainlines_data)

            _save_json(output_dir / "mainlines.json", mainlines_data)
            self._log(f"  Copied mainlines.json ({len(mainlines_data.get('chapters', []))} chapters)")
        else:
            story_entries = self.parser.get_entries_by_types(data, ["story"])
            if story_entries:
                self._log(f"\n[Step 7b] Generating mainlines from {len(story_entries)} story entries...")
                mainlines_data = await self._generate_mainlines(
                    story_entries,
                    maps_data,
                    chars_data=characters_data,
                    output_dir=output_dir, use_direct=use_direct,
                )
                # å…œåº•è¡¥é½ v2 å¿…è¦å­—æ®µï¼ˆç”¨äº legacy æ•°æ®è¿ç§»åˆ° strict-v2ï¼‰
                self._ensure_v2_story_defaults(mainlines_data)
                if settings.narrative_v2_strict_mode:
                    self._validate_mainlines_v2(mainlines_data)
                _save_json(output_dir / "mainlines.json", mainlines_data)
                self._log(f"  Generated mainlines.json ({len(mainlines_data.get('mainlines', []))} volumes, {len(mainlines_data.get('chapters', []))} chapters)")
            else:
                self._log("  No story entries found, skipping mainlines generation")

        # â”€â”€ Step 8: GraphPrefiller â”€â”€
        self._log("\n[Step 8] Running GraphPrefiller...")
        prefiller = GraphPrefiller(output_dir)
        prefill_result = prefiller.run(verbose=self.verbose)

        # â”€â”€ Step 8.5: å¯é€‰è¾¹é‡æ ‡æ³¨ â”€â”€
        if relabel_edges:
            self._log("\n[Step 8.5] Relabeling unknown edges...")
            relabel_stats = await self._relabel_unknown_edges(
                prefill_result, output_dir=output_dir, use_direct=use_direct,
            )
            self._log(f"  Relabeled: {relabel_stats['relabeled']}/{relabel_stats['total_unknown']}")
            stats["relabel_edges"] = relabel_stats

            # åŒæ­¥å›å†™ world_graph.json ä¸­çš„è¾¹
            self._sync_relabeled_edges_to_world_graph(
                output_dir, relabel_stats.get("edge_id_to_relation", {})
            )

        prefiller.save(prefill_result, output_dir)

        stats["prefill_nodes"] = len(prefill_result.nodes)
        stats["prefill_edges"] = len(prefill_result.edges)
        stats["chapters"] = len(prefill_result.chapters_v2)

        # â”€â”€ Step 9: D&D å®ä½“æå–ï¼ˆå¯é€‰ï¼‰ â”€â”€
        if enrich_entities:
            self._log("\n[Step 9] Extracting D&D entities...")
            entity_stats = await self._extract_entities(
                entries=data.entries,
                world_graph_nodes=graph_data.nodes,
                output_dir=output_dir,
                use_direct=use_direct,
            )
            self._log(f"  Monsters: {entity_stats.get('monsters', 0)}, "
                      f"Items: {entity_stats.get('items', 0)}, "
                      f"Skills: {entity_stats.get('skills', 0)}")
            stats["entities"] = entity_stats

        stats["end_time"] = datetime.now().isoformat()

        # â”€â”€ å®Œæˆ â”€â”€
        self._log("\n" + "=" * 50)
        self._log("Unified extraction complete!")
        self._log(f"  Output directory: {output_dir}")
        self._log(f"  maps.json:              {len(maps_data.maps)} maps")
        self._log(f"  characters.json:        {len(characters_data.characters)} characters")
        self._log(f"  world_map.json:         {len(world_map.regions)} regions")
        self._log(f"  character_profiles.json: {len(profiles)} profiles")
        self._log(f"  world_graph.json:       {len(graph_data.nodes)} nodes, {len(graph_data.edges)} edges")
        self._log(f"  prefilled_graph.json:   {len(prefill_result.nodes)} nodes, {len(prefill_result.edges)} edges")
        self._log(f"  chapters_v2.json:       {len(prefill_result.chapters_v2)} chapters")
        if enrich_entities and "entities" in stats:
            es = stats["entities"]
            self._log(f"  monsters.json:          {es.get('monsters', 0)} monsters")
            self._log(f"  items.json:             {es.get('items', 0)} items")
            self._log(f"  skills.json:            {es.get('skills', 0)} skills")

        return stats

    # ---- internal helpers ----

    def _reconcile_characters(
        self,
        characters_data: CharactersData,
        graph_data,
        maps_data: MapsData,
    ) -> CharactersData:
        """å›å¡« world_graph ä¸­æœªè¢« NPC åˆ†ç±»å™¨è¦†ç›–çš„è§’è‰²"""
        # ç°æœ‰è§’è‰²ç´¢å¼•ï¼ˆID + nameï¼‰
        existing_ids = {c.id for c in characters_data.characters}
        existing_names = {c.name for c in characters_data.characters}

        # æœ‰æ•ˆåœ°å›¾ ID é›†åˆ
        valid_maps = {m.id for m in maps_data.maps} if maps_data and maps_data.maps else set()

        backfilled = 0
        for node in graph_data.nodes:
            if node.type != "character":
                continue

            # è§„èŒƒåŒ– IDï¼šstrip "character_" prefix
            raw_id = node.id
            normalized_id = raw_id.removeprefix("character_")

            # è·³è¿‡å·²æœ‰è§’è‰²ï¼ˆæŒ‰ ID æˆ– name åŒ¹é…ï¼‰
            if normalized_id in existing_ids or raw_id in existing_ids:
                continue
            if node.name in existing_names:
                continue

            # ä» properties æå–ä¿¡æ¯
            props = node.properties or {}
            description = props.get("description", "")

            # æ¨æ–­ default_mapï¼šä» graph edges æ‰¾ located_at å…³ç³»
            default_map = None
            if valid_maps:
                for edge in graph_data.edges:
                    if edge.source == raw_id and edge.relation == "located_at":
                        target_map = edge.target.removeprefix("location_")
                        if target_map in valid_maps:
                            default_map = target_map
                            break

            new_char = CharacterInfo(
                id=normalized_id,
                name=node.name,
                tier=NPCTier.SECONDARY,
                default_map=default_map,
                backstory=description,
                importance=node.importance or 0.5,
                tags=["backfilled_from_graph"],
            )
            characters_data.characters.append(new_char)
            existing_ids.add(normalized_id)
            existing_names.add(node.name)
            backfilled += 1

        self._log(f"  Backfilled {backfilled} characters from world_graph")
        return characters_data

    async def _relabel_unknown_edges(
        self,
        prefill_result,
        output_dir: Optional[Path] = None,
        use_direct: bool = False,
    ) -> Dict[str, Any]:
        """é‡æ ‡æ³¨ prefill_result ä¸­ relation ä¸º unknown/related/"" çš„è¾¹

        Args:
            prefill_result: GraphPrefiller çš„è¾“å‡º
            output_dir: è¾“å‡ºç›®å½•ï¼ˆbatch æ¨¡å¼ç”¨äºå­˜æ”¾ä¸´æ—¶æ–‡ä»¶ï¼‰
            use_direct: True èµ°é€æ‰¹ LLM ç›´æ¥è°ƒç”¨ï¼ŒFalse èµ° Batch API
        """
        # æ‰¾å‡ºéœ€è¦é‡æ ‡æ³¨çš„è¾¹
        unknown_edges = [
            e for e in prefill_result.edges
            if e.relation in ("unknown", "related", "")
        ]
        stats: Dict[str, Any] = {
            "total_unknown": len(unknown_edges),
            "relabeled": 0,
            "edge_id_to_relation": {},
        }

        if not unknown_edges:
            self._log("  No unknown edges found")
            return stats

        # æ„å»ºèŠ‚ç‚¹æŸ¥æ‰¾è¡¨
        node_lookup = {n.id: {"name": n.name, "type": n.type} for n in prefill_result.nodes}

        prompt_path = Path(__file__).parent / "prompts" / "edge_relabeling.md"
        if not prompt_path.exists():
            self._log(f"  Warning: Prompt template not found: {prompt_path}")
            return stats

        prompt_template = prompt_path.read_text(encoding="utf-8")
        edge_id_to_relation: Dict[str, str] = {}

        # åˆ†æ‰¹æ„å»º promptï¼Œæ¯æ‰¹ 30 æ¡
        batch_size = 30
        batch_prompts: List[tuple] = []  # (key, prompt)

        for batch_start in range(0, len(unknown_edges), batch_size):
            batch = unknown_edges[batch_start:batch_start + batch_size]
            batch_num = batch_start // batch_size + 1

            edge_lines = []
            for edge in batch:
                source_info = node_lookup.get(edge.source, {"name": edge.source, "type": "?"})
                target_info = node_lookup.get(edge.target, {"name": edge.target, "type": "?"})
                edge_lines.append(
                    f"- edge_id: {edge.id} | "
                    f"source: {source_info['name']} (type={source_info['type']}) | "
                    f"target: {target_info['name']} (type={target_info['type']})"
                )

            edges_batch = "\n".join(edge_lines)
            prompt = prompt_template.format(edges_batch=edges_batch)
            batch_prompts.append((f"batch_{batch_num}", prompt))

        def _parse_relabel_result(text: str) -> List[Dict]:
            """ä»åŸå§‹æ–‡æœ¬è§£æè¾¹æ ‡æ³¨ç»“æœ"""
            try:
                parsed = json.loads(text)
            except json.JSONDecodeError:
                match = re.search(r'[\[\{][\s\S]*[\]\}]', text)
                if match:
                    try:
                        parsed = json.loads(match.group(0))
                    except json.JSONDecodeError:
                        return []
                else:
                    return []

            if isinstance(parsed, list):
                return parsed
            if isinstance(parsed, dict) and "edges" in parsed:
                return parsed["edges"]
            return []

        if use_direct:
            # â”€â”€ ç›´æ¥è°ƒç”¨æ¨¡å¼ â”€â”€
            from app.services.llm_service import LLMService
            llm = LLMService()
            total_batches = len(batch_prompts)

            for key, prompt in batch_prompts:
                self._log(f"    {key}/{total_batches} ...")
                try:
                    result = await llm.generate_simple(prompt, model_override=self.model)
                    items = _parse_relabel_result(result)
                    for item in items:
                        eid = item.get("edge_id", "")
                        rel = item.get("relation", "related_to")
                        if eid:
                            edge_id_to_relation[eid] = rel
                    self._log(f"      Labeled {len(items)} edges")
                except Exception as exc:
                    self._log(f"      Error: {exc}")
        else:
            # â”€â”€ Batch API æ¨¡å¼ â”€â”€
            self._log(f"  Submitting {len(batch_prompts)} batches to Batch API...")
            temp_dir = (output_dir or Path(".")) / "batch_temp"
            raw_results = self.batch_runner.run_batch(
                requests=batch_prompts,
                temp_dir=temp_dir,
                display_name="edge-relabeling",
            )
            for key, text in raw_results.items():
                items = _parse_relabel_result(text)
                for item in items:
                    eid = item.get("edge_id", "")
                    rel = item.get("relation", "related_to")
                    if eid:
                        edge_id_to_relation[eid] = rel

        # åº”ç”¨æ ‡æ³¨åˆ° prefill_result.edges
        for edge in prefill_result.edges:
            if edge.id in edge_id_to_relation:
                edge.relation = edge_id_to_relation[edge.id]

        stats["relabeled"] = len(edge_id_to_relation)
        stats["edge_id_to_relation"] = edge_id_to_relation
        return stats

    def _sync_relabeled_edges_to_world_graph(
        self,
        output_dir: Path,
        edge_id_to_relation: Dict[str, str],
    ) -> None:
        """å°†è¾¹é‡æ ‡æ³¨ç»“æœåŒæ­¥å›å†™åˆ° world_graph.json"""
        if not edge_id_to_relation:
            return

        wg_path = output_dir / "world_graph.json"
        if not wg_path.exists():
            return

        wg_data = json.loads(wg_path.read_text(encoding="utf-8"))
        updated = 0
        for edge in wg_data.get("edges", []):
            eid = edge.get("id", "")
            if eid in edge_id_to_relation:
                edge["relation"] = edge_id_to_relation[eid]
                updated += 1

        if updated:
            _save_json(wg_path, wg_data)
            self._log(f"  Synced {updated} relabeled edges to world_graph.json")

    async def _extract_entities(
        self,
        entries: list,
        world_graph_nodes: list,
        output_dir: Path,
        use_direct: bool = False,
    ) -> Dict[str, int]:
        """ä»æ¡ç›®ä¸­æå– D&D å®ä½“æ•°æ®ï¼ˆæ€ªç‰©ã€ç‰©å“ã€æŠ€èƒ½ï¼‰

        Args:
            use_direct: True èµ°é€æ¡ LLM ç›´æ¥è°ƒç”¨ï¼ŒFalse èµ° Batch API
        """
        # æ„å»ºå·²æœ‰èŠ‚ç‚¹å±æ€§çš„æŸ¥æ‰¾è¡¨
        existing_nodes = {}
        for node in world_graph_nodes:
            existing_nodes[node.id] = {
                "properties": node.properties or {},
            }

        # åˆ†ç±»æ¡ç›®
        monsters_raw, items_raw, skills_raw = [], [], []
        for entry in entries:
            entry_keys = entry.key if isinstance(entry.key, list) else [entry.key] if entry.key else []
            entry_name = entry.comment or ""
            entry_type = entry.entry_type or ""
            entry_group = entry.group or ""

            all_text = f"{entry_name} {' '.join(entry_keys)} {entry_type} {entry_group}".lower()

            if any(kw in all_text for kw in ["monster", "æ€ªç‰©", "é­”ç‰©", "boss", "æ•Œäºº"]):
                monsters_raw.append(entry)
            elif any(kw in all_text for kw in ["item", "ç‰©å“", "æ­¦å™¨", "é˜²å…·", "é“å…·", "è£…å¤‡"]):
                items_raw.append(entry)
            elif any(kw in all_text for kw in ["skill", "æŠ€èƒ½", "æ³•æœ¯", "é­”æ³•", "å¥‡è¿¹", "èƒ½åŠ›"]):
                skills_raw.append(entry)

        self._log(f"  Categorized: {len(monsters_raw)} monsters, {len(items_raw)} items, {len(skills_raw)} skills")

        stats = {"monsters": 0, "items": 0, "skills": 0}

        # æ„å»º (category_name, prompt_file, entries) ä¸‰å…ƒç»„
        categories = [
            ("monster", "monster_extraction.md", monsters_raw),
            ("item", "item_extraction.md", items_raw),
            ("skill", "skill_extraction.md", skills_raw),
        ]

        def _build_entity_prompts(category_name, prompt_file, category_entries):
            """ä¸ºä¸€ä¸ªç±»åˆ«çš„å…¨éƒ¨æ¡ç›®æ„å»º (key, prompt) åˆ—è¡¨"""
            prompt_path = Path(__file__).parent / "prompts" / prompt_file
            if not prompt_path.exists():
                self._log(f"  Warning: {prompt_path} not found, skipping {category_name}")
                return []

            prompt_template = prompt_path.read_text(encoding="utf-8")
            prompts = []

            for i, entry in enumerate(category_entries):
                entry_name = entry.comment or ""
                entry_id = entry_name.replace(" ", "_").lower() or f"{category_name}_{i}"
                entry_desc = (entry.content or "")[:3000]

                existing = existing_nodes.get(entry_id, {})
                existing_props = existing.get("properties", {}) if existing else {}
                existing_text = json.dumps(existing_props, ensure_ascii=False) if existing_props else "æ— "

                prompt = prompt_template.format(
                    **{
                        f"{category_name}_id": entry_id,
                        f"{category_name}_name": entry_name,
                        f"{category_name}_description": entry_desc,
                        "existing_properties": existing_text,
                    }
                )
                prompts.append((f"{category_name}_{i}", prompt))

            return prompts

        def _parse_entity_text(text: str):
            """è§£æå•ä¸ªå®ä½“çš„ JSON ç»“æœ"""
            try:
                return json.loads(text)
            except json.JSONDecodeError:
                match = re.search(r'\{[\s\S]*\}', text)
                if match:
                    try:
                        return json.loads(match.group(0))
                    except json.JSONDecodeError:
                        return None
                return None

        if use_direct:
            # â”€â”€ ç›´æ¥è°ƒç”¨æ¨¡å¼ï¼šé€æ¡å¤„ç† â”€â”€
            from app.services.llm_service import LLMService
            llm = LLMService()

            for category_name, prompt_file, category_entries in categories:
                if not category_entries:
                    continue
                self._log(f"\n  Processing {len(category_entries)} {category_name}s (direct)...")
                prompts = _build_entity_prompts(category_name, prompt_file, category_entries)
                results = []
                for i, (key, prompt) in enumerate(prompts):
                    entry_name = category_entries[i].comment or key
                    self._log(f"    [{i+1}/{len(prompts)}] {entry_name}...")
                    try:
                        result = await llm.generate_simple(prompt, model_override=self.model)
                        parsed = llm.parse_json(result)
                        if parsed:
                            results.append(parsed)
                        else:
                            self._log(f"      Parse failed for {entry_name}")
                    except Exception as exc:
                        self._log(f"      Error: {exc}")

                plural = f"{category_name}s"
                _save_json(output_dir / f"{plural}.json", {plural: results})
                stats[plural] = len(results)
        else:
            # â”€â”€ Batch API æ¨¡å¼ï¼šæ‰€æœ‰ç±»åˆ«åˆå¹¶ä¸ºä¸€ä¸ª batch â”€â”€
            all_prompts: List[tuple] = []
            category_ranges: Dict[str, int] = {}  # category_name -> count

            for category_name, prompt_file, category_entries in categories:
                if not category_entries:
                    continue
                prompts = _build_entity_prompts(category_name, prompt_file, category_entries)
                category_ranges[category_name] = len(prompts)
                all_prompts.extend(prompts)

            if all_prompts:
                self._log(f"\n  Submitting {len(all_prompts)} entity extractions to Batch API...")
                temp_dir = output_dir / "batch_temp"
                raw_results = self.batch_runner.run_batch(
                    requests=all_prompts,
                    temp_dir=temp_dir,
                    display_name="entity-extraction",
                )

                # æŒ‰ key å‰ç¼€æ‹†åˆ†ç»“æœ
                for category_name, _, _ in categories:
                    plural = f"{category_name}s"
                    count = category_ranges.get(category_name, 0)
                    if count == 0:
                        continue
                    results = []
                    for i in range(count):
                        key = f"{category_name}_{i}"
                        text = raw_results.get(key, "")
                        if text:
                            parsed = _parse_entity_text(text)
                            if parsed:
                                results.append(parsed)

                    _save_json(output_dir / f"{plural}.json", {plural: results})
                    stats[plural] = len(results)

        return stats

    async def _generate_mainlines(
        self,
        story_entries: list,
        maps_data: MapsData,
        chars_data: Any = None,
        output_dir: Optional[Path] = None,
        use_direct: bool = False,
    ) -> Dict[str, Any]:
        """ä»æ•…äº‹ç±»æ¡ç›®ç”Ÿæˆ mainlines.json

        Phase 1ï¼ˆçº¯è§„åˆ™ï¼‰ï¼šä» entry.comment æå–å·/ç« ç»“æ„ + ç« èŠ‚åˆ†ç±» + æ­£åˆ™æå–
        Phase 2ï¼ˆLLMï¼‰ï¼šå¢é‡å¡«å……ç¼ºå¤±çš„ available_mapsã€objectives ç­‰
        """
        # Phase 1: ç”¨æ­£åˆ™æå–å·/ç« ç»“æ„
        volumes: Dict[str, Dict[str, Any]] = {}   # vol_id -> {name, chapters: []}
        chapters: List[Dict[str, Any]] = []

        # å¯¹ story_entries æŒ‰ comment æ’åºç¡®ä¿é¡ºåº
        sorted_entries = sorted(story_entries, key=lambda e: e.order)

        vol_pattern = re.compile(r'ç¬¬(\d+|[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾]+)å·')
        ch_pattern = re.compile(r'ç¬¬(\d+|[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾]+)ç« ')

        # ç« èŠ‚åˆ†ç±»æ­£åˆ™
        metadata_keywords = ["çŠ¶æ€æ ", "å‰§æƒ…ç³»ç»Ÿ", "å‰§æƒ…åˆå§‹åŒ–", "ç« èŠ‚ç®¡ç†å™¨"]
        volume_idx_pattern = re.compile(r"^[ğŸ“–ğŸ“š\s]*ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+å·[^ç« ]*$")

        # æ­£åˆ™æå–ç›®æ ‡å’Œäº‹ä»¶
        objective_pattern = re.compile(r"(?:ä¸»è¦ç›®æ ‡|ç« èŠ‚ç›®æ ‡)[ï¼š:]\s*(.+?)(?:\n|$)")
        event_list_pattern = re.compile(r"<ç¬¬\d+ç« äº‹ä»¶åˆ—è¡¨>(.*?)</", re.DOTALL)
        event_line_pattern = re.compile(r"ç« èŠ‚äº‹ä»¶[ï¼š:]\s*(.+?)(?:\n|$)")

        current_vol_id = "vol_1"
        current_vol_name = "ç¬¬ä¸€å·"
        classify_stats = {"metadata": 0, "volume_index": 0, "story": 0}

        for entry in sorted_entries:
            comment = entry.comment or ""
            content_preview = (entry.content or "")[:500]

            # â”€â”€ ç« èŠ‚ç±»å‹åˆ†ç±» â”€â”€
            if any(kw in comment for kw in metadata_keywords):
                classify_stats["metadata"] += 1
                continue  # è·³è¿‡å…ƒæ•°æ®æ¡ç›®

            comment_stripped = re.sub(r"[ğŸ“–ğŸ“š\s]", "", comment)
            if volume_idx_pattern.match(comment) or (
                re.match(r"ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+å·", comment_stripped)
                and "ç« " not in comment_stripped
            ):
                # ä»…ä½œä¸ºå·æ ‡é¢˜ï¼Œæ›´æ–°å·ä¿¡æ¯ä½†ä¸ç”Ÿæˆç« èŠ‚
                vol_match = vol_pattern.search(comment)
                if vol_match:
                    vol_num = vol_match.group(1)
                    if vol_num.isdigit():
                        vol_num_int = int(vol_num)
                    else:
                        vol_num_int = self._cn_num_to_int(vol_num)
                    current_vol_id = f"vol_{vol_num_int}"
                    current_vol_name = comment.split(" - ")[0].strip() if " - " in comment else comment.strip()
                    if current_vol_id not in volumes:
                        volumes[current_vol_id] = {
                            "id": current_vol_id,
                            "name": current_vol_name,
                            "description": content_preview,
                            "chapters": [],
                        }
                classify_stats["volume_index"] += 1
                continue

            # æ•…äº‹ç« èŠ‚
            ch_type = "story"
            classify_stats["story"] += 1

            # æ£€æµ‹å·å·
            vol_match = vol_pattern.search(comment)
            if vol_match:
                vol_num = vol_match.group(1)
                # æ•°å­—åŒ–å·å·
                if vol_num.isdigit():
                    vol_num_int = int(vol_num)
                else:
                    vol_num_int = self._cn_num_to_int(vol_num)
                current_vol_id = f"vol_{vol_num_int}"
                current_vol_name = comment.split(" - ")[0].strip() if " - " in comment else comment.strip()

            # æ£€æµ‹ç« å·
            ch_match = ch_pattern.search(comment)
            if ch_match:
                ch_num = ch_match.group(1)
                if ch_num.isdigit():
                    ch_num_int = int(ch_num)
                else:
                    ch_num_int = self._cn_num_to_int(ch_num)
            else:
                # æ— ç« å·æ ‡è®°ï¼ŒæŒ‰åºç¼–å·
                ch_num_int = len(chapters) + 1

            # æå–ç« èŠ‚åç§°
            ch_name = comment.strip()
            if " - " in comment:
                ch_name = comment.split(" - ", 1)[1].strip()

            # ç¡®ä¿å·å­˜åœ¨
            if current_vol_id not in volumes:
                volumes[current_vol_id] = {
                    "id": current_vol_id,
                    "name": current_vol_name,
                    "description": "",
                    "chapters": [],
                }

            # æå–å·å·æ•°å­—ç”¨äºç« èŠ‚ IDï¼ˆç¡®ä¿å”¯ä¸€æ€§ï¼‰
            vol_num_str = current_vol_id.removeprefix("vol_")
            ch_id = f"ch_{vol_num_str}_{ch_num_int}"
            existing_ids = {c["id"] for c in chapters}
            if ch_id in existing_ids:
                suffix = 2
                while f"{ch_id}_{suffix}" in existing_ids:
                    suffix += 1
                self._log(f"    ID collision: {ch_id} â†’ {ch_id}_{suffix}")
                ch_id = f"{ch_id}_{suffix}"

            # â”€â”€ æ­£åˆ™æå–ç›®æ ‡ â”€â”€
            objectives = []
            for match in objective_pattern.finditer(content_preview):
                obj_text = match.group(1).strip()
                if obj_text:
                    objectives.append(obj_text)

            # â”€â”€ æ­£åˆ™æå–äº‹ä»¶ â”€â”€
            events = []
            for match in event_list_pattern.finditer(content_preview):
                event_block = match.group(1)
                for line in event_block.strip().splitlines():
                    line = line.strip(" -*Â·")
                    if line:
                        events.append(f"{ch_id}_{line.replace(' ', '_')[:30]}")

            for match in event_line_pattern.finditer(content_preview):
                event_text = match.group(1).strip()
                if event_text:
                    for ev in re.split(r"[,ï¼Œã€;ï¼›]", event_text):
                        ev = ev.strip()
                        if ev:
                            events.append(f"{ch_id}_{ev.replace(' ', '_')[:30]}")

            completion_conditions = {}
            if events:
                completion_conditions["events_required"] = events

            chapter_info = {
                "id": ch_id,
                "mainline_id": current_vol_id,
                "name": ch_name,
                "type": ch_type,
                "description": content_preview,
                "available_maps": [],
                "objectives": objectives,
                "trigger_conditions": {},
                "completion_conditions": completion_conditions,
            }
            chapters.append(chapter_info)
            volumes[current_vol_id]["chapters"].append(ch_id)

        self._log(f"  Classification: metadata={classify_stats['metadata']}, "
                  f"volume_index={classify_stats['volume_index']}, story={classify_stats['story']}")

        # Phase 2: å¢é‡ LLM å¡«å…… available_maps å’Œ objectives
        if chapters and maps_data and maps_data.maps:
            try:
                chapters = await self._enrich_mainlines_incremental(
                    chapters, volumes, maps_data,
                    output_dir=output_dir, use_direct=use_direct,
                )
            except Exception as e:
                import traceback
                self._log(f"  Warning: Incremental LLM enrichment failed, trying batch fallback: {e}")
                self._log(f"  Traceback: {traceback.format_exc()}")
                if settings.narrative_v2_strict_mode:
                    raise
                try:
                    chapters = await self._enrich_mainlines_with_llm(
                        chapters, volumes, maps_data
                    )
                except Exception as e2:
                    self._log(f"  Warning: Batch LLM enrichment also failed: {e2}")

        # Phase 3: ç« èŠ‚ç¼–æ’æå–ï¼ˆevents, transitions, pacingï¼‰
        if chapters and maps_data:
            try:
                chapters = await self._extract_chapter_orchestration(
                    chapters, volumes, maps_data, chars_data=chars_data,
                    output_dir=output_dir, use_direct=use_direct,
                )
            except Exception as e:
                import traceback
                self._log(f"  ERROR: Chapter orchestration extraction failed: {e}")
                self._log(f"  Traceback: {traceback.format_exc()}")
                if settings.narrative_v2_strict_mode:
                    raise

        mainlines_list = list(volumes.values())
        return {
            "mainlines": mainlines_list,
            "chapters": chapters,
        }

    async def _enrich_mainlines_with_llm(
        self,
        chapters: List[Dict[str, Any]],
        volumes: Dict[str, Dict[str, Any]],
        maps_data: MapsData,
    ) -> List[Dict[str, Any]]:
        """ç”¨ LLM ä¸ºæ¯ä¸ªç« èŠ‚å¡«å…… available_maps å’Œ objectives"""
        prompt_path = Path(__file__).parent / "prompts" / "mainline_extraction.md"
        prompt_template = prompt_path.read_text(encoding="utf-8")

        known_maps = "\n".join(f"- {m.id}: {m.name}" for m in maps_data.maps)

        # æ„å»ºç« èŠ‚è¾“å…¥æ‘˜è¦
        chapters_input_parts = []
        for ch in chapters:
            chapters_input_parts.append(
                f"### {ch['id']} ({ch['mainline_id']}): {ch['name']}\n"
                f"{ch['description'][:300]}"
            )
        chapters_input = "\n\n".join(chapters_input_parts)

        prompt = prompt_template.replace(
            "{known_maps}", known_maps
        ).replace(
            "{chapters_input}", chapters_input
        )

        client = genai.Client(api_key=self.api_key or settings.gemini_api_key)
        config = types.GenerateContentConfig(
            response_mime_type="application/json",
            temperature=0.2,
            max_output_tokens=65536,
        )

        response = client.models.generate_content(
            model=self.model,
            contents=prompt,
            config=config,
        )

        # æå– JSON
        text = ""
        if hasattr(response, 'candidates') and response.candidates:
            for part in response.candidates[0].content.parts:
                if hasattr(part, 'text') and part.text:
                    if not (hasattr(part, 'thought') and part.thought):
                        text += part.text

        if not text:
            return chapters

        try:
            enriched = json.loads(text)
        except json.JSONDecodeError:
            # å°è¯•æå– JSON å—
            match = re.search(r'\{[\s\S]*\}', text)
            if match:
                enriched = json.loads(match.group(0))
            else:
                return chapters

        # åˆå¹¶ LLM ç»“æœåˆ°ç« èŠ‚
        llm_chapters = {ch["id"]: ch for ch in enriched.get("chapters", [])}
        valid_map_ids = {m.id for m in maps_data.maps}

        for ch in chapters:
            if ch["id"] in llm_chapters:
                llm_ch = llm_chapters[ch["id"]]
                # åªä¿ç•™æœ‰æ•ˆåœ°å›¾ ID
                ch["available_maps"] = [
                    m for m in llm_ch.get("available_maps", [])
                    if m in valid_map_ids
                ]
                ch["objectives"] = llm_ch.get("objectives", [])
                ch["trigger_conditions"] = llm_ch.get("trigger_conditions", {})
                ch["completion_conditions"] = llm_ch.get("completion_conditions", {})

        # åˆå¹¶å·çº§ä¿¡æ¯
        llm_mainlines = {ml["id"]: ml for ml in enriched.get("mainlines", [])}
        for vol_id, vol in volumes.items():
            if vol_id in llm_mainlines:
                vol["description"] = llm_mainlines[vol_id].get("description", vol["description"])

        return chapters

    async def _enrich_mainlines_incremental(
        self,
        chapters: List[Dict[str, Any]],
        volumes: Dict[str, Dict[str, Any]],
        maps_data: MapsData,
        output_dir: Optional[Path] = None,
        use_direct: bool = False,
    ) -> List[Dict[str, Any]]:
        """å¢é‡ LLM å¢å¼ºï¼šåªå¤„ç† type==story ä¸”ç¼ºå¤±å­—æ®µçš„ç« èŠ‚

        Args:
            output_dir: è¾“å‡ºç›®å½•ï¼ˆbatch æ¨¡å¼ç”¨äºå­˜æ”¾ä¸´æ—¶æ–‡ä»¶ï¼‰
            use_direct: True èµ°é€ç«  LLM ç›´æ¥è°ƒç”¨ï¼ŒFalse èµ° Batch API
        """
        prompt_path = Path(__file__).parent / "prompts" / "mainline_enrichment.md"
        if not prompt_path.exists():
            self._log(f"  Warning: Prompt template not found: {prompt_path}")
            return chapters

        prompt_template = prompt_path.read_text(encoding="utf-8")
        known_maps = "\n".join(f"- {m.id}: {m.name}" for m in maps_data.maps)
        valid_map_ids = {m.id for m in maps_data.maps}

        # ç­›é€‰éœ€è¦ LLM å¢å¼ºçš„ç« èŠ‚
        need_enrich = [
            ch for ch in chapters
            if ch.get("type") == "story"
            and (not ch.get("available_maps") or not ch.get("objectives"))
        ]

        if not need_enrich:
            self._log("  All story chapters already have maps and objectives, skipping LLM")
            return chapters

        self._log(f"  Chapters needing LLM enrichment: {len(need_enrich)}/{len(chapters)}")

        # æ„å»ºæ‰€æœ‰ç« èŠ‚çš„ prompt
        chapter_prompts: List[tuple] = []  # (key, prompt)
        for ch in need_enrich:
            ch_id = ch.get("id", "unknown")

            existing_objectives = ch.get("objectives", [])
            if isinstance(existing_objectives, list) and existing_objectives:
                obj_text = "\n".join(
                    f"- {o}" if isinstance(o, str) else f"- {o.get('description', '')}"
                    for o in existing_objectives
                )
            else:
                obj_text = "æ— "

            prompt = prompt_template.format(
                chapter_id=ch_id,
                chapter_name=ch.get("name", ""),
                chapter_description=ch.get("description", "")[:2000],
                known_maps=known_maps,
                existing_objectives=obj_text,
            )
            chapter_prompts.append((f"ch_{ch_id}", prompt))

        def _apply_enrichment(ch: Dict[str, Any], parsed: Dict[str, Any]) -> bool:
            """å°† LLM ç»“æœåˆå¹¶åˆ°ç« èŠ‚ï¼Œè¿”å›æ˜¯å¦æˆåŠŸ"""
            if not parsed:
                return False
            if parsed.get("available_maps") and not ch.get("available_maps"):
                ch["available_maps"] = [
                    m for m in parsed["available_maps"]
                    if m in valid_map_ids
                ]
            if parsed.get("objectives") and not ch.get("objectives"):
                ch["objectives"] = parsed["objectives"]
            if parsed.get("completion_conditions"):
                if "completion_conditions" not in ch:
                    ch["completion_conditions"] = {}
                cc = parsed["completion_conditions"]
                if cc.get("events_required") and not ch["completion_conditions"].get("events_required"):
                    ch["completion_conditions"]["events_required"] = cc["events_required"]
            return True

        enriched_count = 0

        if use_direct:
            # â”€â”€ ç›´æ¥è°ƒç”¨æ¨¡å¼ â”€â”€
            from app.services.llm_service import LLMService
            llm = LLMService()

            for i, (ch, (key, prompt)) in enumerate(zip(need_enrich, chapter_prompts)):
                ch_id = ch.get("id", "unknown")
                self._log(f"    [{i+1}/{len(need_enrich)}] Enriching {ch_id}...")
                try:
                    result = await llm.generate_simple(prompt, model_override=self.model)
                    parsed = llm.parse_json(result)
                    if _apply_enrichment(ch, parsed):
                        enriched_count += 1
                    else:
                        self._log(f"      Parse failed for {ch_id}")
                except Exception as exc:
                    self._log(f"      Error enriching {ch_id}: {exc}")
        else:
            # â”€â”€ Batch API æ¨¡å¼ â”€â”€
            self._log(f"  Submitting {len(chapter_prompts)} chapters to Batch API...")
            temp_dir = (output_dir or Path(".")) / "batch_temp"
            raw_results = self.batch_runner.run_batch(
                requests=chapter_prompts,
                temp_dir=temp_dir,
                display_name="mainline-enrichment",
            )

            # æŒ‰ key åŒ¹é…å›ç« èŠ‚ï¼ˆé˜²å¾¡æ€§å»é‡ï¼‰
            key_to_ch: Dict[str, Dict] = {}
            for ch in need_enrich:
                key = f"ch_{ch.get('id', 'unknown')}"
                if key in key_to_ch:
                    self._log(f"    WARNING: Duplicate enrichment key {key}, skipping")
                    continue
                key_to_ch[key] = ch
            for key, text in raw_results.items():
                ch = key_to_ch.get(key)
                if not ch:
                    continue
                try:
                    parsed = json.loads(text)
                except json.JSONDecodeError:
                    match = re.search(r'\{[\s\S]*\}', text)
                    if match:
                        try:
                            parsed = json.loads(match.group(0))
                        except json.JSONDecodeError:
                            continue
                    else:
                        continue

                if _apply_enrichment(ch, parsed):
                    enriched_count += 1

        self._log(f"  LLM enriched: {enriched_count}/{len(need_enrich)}")
        return chapters

    async def _extract_chapter_orchestration(
        self,
        chapters: List[Dict[str, Any]],
        volumes: Dict[str, Dict[str, Any]],
        maps_data: MapsData,
        chars_data: Any = None,
        output_dir: Optional[Path] = None,
        use_direct: bool = False,
    ) -> List[Dict[str, Any]]:
        """Phase 3: ä»ç« èŠ‚æè¿°æå–å‰§æƒ…ç¼–æ’æ•°æ®ï¼ˆevents, transitions, pacingï¼‰

        Args:
            chapters: ç« èŠ‚åˆ—è¡¨
            volumes: å·å­—å…¸
            maps_data: åœ°å›¾æ•°æ®
            chars_data: è§’è‰²æ•°æ®ï¼ˆå¯é€‰ï¼‰
            output_dir: è¾“å‡ºç›®å½•ï¼ˆbatch æ¨¡å¼ä¸´æ—¶æ–‡ä»¶ï¼‰
            use_direct: True èµ°ç›´æ¥è°ƒç”¨ï¼ŒFalse èµ° Batch API
        """
        prompt_path = Path(__file__).parent / "prompts" / "chapter_orchestration.md"
        if not prompt_path.exists():
            self._log(f"  Warning: Prompt template not found: {prompt_path}")
            return chapters

        prompt_template = prompt_path.read_text(encoding="utf-8")

        # ç­›é€‰ story ç±»å‹çš„ç« èŠ‚
        story_chapters = [ch for ch in chapters if ch.get("type") == "story"]
        if not story_chapters:
            self._log("  No story chapters for orchestration extraction")
            return chapters

        self._log(f"\n  [Phase 3] Extracting chapter orchestration for {len(story_chapters)} chapters...")

        # æ„å»ºå·²çŸ¥åœ°å›¾å’Œ NPC åˆ—è¡¨
        known_maps = "\n".join(f"- {m.id}: {m.name}" for m in maps_data.maps) if maps_data and maps_data.maps else "æ— "
        known_npcs = "æ— "
        if chars_data and hasattr(chars_data, "characters"):
            known_npcs = "\n".join(
                f"- {c.id}: {c.name}" for c in chars_data.characters[:50]
            ) or "æ— "

        # æŒ‰ç« èŠ‚é¡ºåºæ„å»º promptï¼Œè·Ÿè¸ªå‰åºäº‹ä»¶
        chapter_prompts: List[tuple] = []  # (key, prompt)
        previous_events_by_chapter: Dict[str, str] = {}

        # é¢„è®¡ç®—æ¯ä¸ªç« èŠ‚çš„å‰åºç« èŠ‚ ID
        chapter_order = {ch["id"]: i for i, ch in enumerate(story_chapters)}
        prev_chapter_ids: Dict[str, Optional[str]] = {}
        for i, ch in enumerate(story_chapters):
            if i > 0:
                prev_chapter_ids[ch["id"]] = story_chapters[i - 1]["id"]
            else:
                prev_chapter_ids[ch["id"]] = None

        for ch in story_chapters:
            ch_id = ch.get("id", "unknown")
            prev_id = prev_chapter_ids.get(ch_id)

            # æ„å»ºå‰åºäº‹ä»¶ä¿¡æ¯
            if prev_id and prev_id in previous_events_by_chapter:
                previous_events = previous_events_by_chapter[prev_id]
            elif prev_id:
                # ç”¨å‰åºç« èŠ‚çš„äº‹ä»¶ä½œä¸ºå‚è€ƒ
                prev_ch = next((c for c in chapters if c.get("id") == prev_id), None)
                if prev_ch and prev_ch.get("completion_conditions", {}).get("events_required"):
                    events_list = prev_ch["completion_conditions"]["events_required"]
                    previous_events = "\n".join(f"- {eid}" for eid in events_list)
                else:
                    previous_events = f"- {prev_id}_event_finalï¼ˆå‰ä¸€ç« æœ€ç»ˆäº‹ä»¶ï¼‰"
            else:
                previous_events = "æ— ï¼ˆè¿™æ˜¯ç¬¬ä¸€ç« ï¼‰"

            prompt = prompt_template.format(
                chapter_id=ch_id,
                chapter_name=ch.get("name", ""),
                chapter_description=ch.get("description", "")[:3000],
                known_maps=known_maps,
                known_npcs=known_npcs,
                previous_events=previous_events,
            )
            orch_key = f"orch_{ch_id}"
            existing_keys = {k for k, _ in chapter_prompts}
            if orch_key in existing_keys:
                self._log(f"    WARNING: Duplicate orchestration key {orch_key}, appending suffix")
                orch_key = f"{orch_key}__dup"
            chapter_prompts.append((orch_key, prompt))

            # ä¸ºåç»­ç« èŠ‚é¢„å¡«å‰åºäº‹ä»¶å ä½
            previous_events_by_chapter[ch_id] = f"- {ch_id}_event_1ï¼ˆå°†ç”± LLM ç”Ÿæˆï¼‰"

        def _parse_orchestration(text: str) -> Optional[Dict[str, Any]]:
            """è§£æç¼–æ’ JSON ç»“æœ"""
            try:
                return json.loads(text)
            except json.JSONDecodeError:
                match = re.search(r'\{[\s\S]*\}', text)
                if match:
                    try:
                        return json.loads(match.group(0))
                    except json.JSONDecodeError:
                        return None
                return None

        def _apply_orchestration(ch: Dict[str, Any], parsed: Dict[str, Any]) -> bool:
            """å°†ç¼–æ’ç»“æœåˆå¹¶åˆ°ç« èŠ‚"""
            if not parsed:
                return False
            events_raw = parsed.get("events", [])
            transitions_raw = parsed.get("transitions", [])
            pacing_raw = parsed.get("pacing", {})

            ch["events"] = events_raw if isinstance(events_raw, list) else []
            ch["transitions"] = transitions_raw if isinstance(transitions_raw, list) else []
            ch["pacing"] = pacing_raw if isinstance(pacing_raw, dict) else {}
            ch["entry_conditions"] = parsed.get("entry_conditions")
            tags_raw = parsed.get("tags", [])
            ch["tags"] = tags_raw if isinstance(tags_raw, list) else []
            return True

        orchestrated_count = 0
        ch_id_to_chapter: Dict[str, Dict] = {}
        for ch in chapters:
            cid = ch["id"]
            if cid in ch_id_to_chapter:
                self._log(f"    WARNING: Duplicate chapter ID {cid} in orchestration mapping")
                continue
            ch_id_to_chapter[cid] = ch

        if use_direct:
            # â”€â”€ ç›´æ¥è°ƒç”¨æ¨¡å¼ â”€â”€
            from app.services.llm_service import LLMService
            llm = LLMService()

            for i, (key, prompt) in enumerate(chapter_prompts):
                ch_id = key.removeprefix("orch_")
                ch = ch_id_to_chapter.get(ch_id)
                if not ch:
                    continue
                self._log(f"    [{i+1}/{len(chapter_prompts)}] Orchestrating {ch_id}...")
                try:
                    result = await llm.generate_simple(prompt, model_override=self.model)
                    parsed = _parse_orchestration(result)
                    if _apply_orchestration(ch, parsed):
                        orchestrated_count += 1
                    else:
                        self._log(f"      Parse failed for {ch_id}")
                except Exception as exc:
                    self._log(f"      Error: {exc}")
        else:
            # â”€â”€ Batch API æ¨¡å¼ â”€â”€
            self._log(f"  Submitting {len(chapter_prompts)} chapters to Batch API for orchestration...")
            temp_dir = (output_dir or Path(".")) / "batch_temp"
            raw_results = self.batch_runner.run_batch(
                requests=chapter_prompts,
                temp_dir=temp_dir,
                display_name="chapter-orchestration",
            )

            for key, text in raw_results.items():
                ch_id = key.removeprefix("orch_")
                ch = ch_id_to_chapter.get(ch_id)
                if not ch:
                    continue
                parsed = _parse_orchestration(text)
                if _apply_orchestration(ch, parsed):
                    orchestrated_count += 1

        # strict-v2: å³ä½¿è§£æå¤±è´¥ä¹Ÿå†™å…¥ç©ºé”®ï¼Œé¿å…ä¸‹æ¸¸å­—æ®µç¼ºå¤±
        for ch in story_chapters:
            ch.setdefault("events", [])
            ch.setdefault("transitions", [])
            ch.setdefault("pacing", {})
            ch.setdefault("entry_conditions", None)
            ch.setdefault("tags", [])

        self._log(f"  Chapter orchestration extracted: {orchestrated_count}/{len(story_chapters)}")
        return chapters

    @staticmethod
    def _build_linear_chapter_graph(chapter_ids: List[str]) -> Dict[str, List[str]]:
        """æŒ‰ç« èŠ‚é¡ºåºæ„å»ºçº¿æ€§ chapter_graphã€‚"""
        graph: Dict[str, List[str]] = {}
        for i in range(len(chapter_ids) - 1):
            graph[chapter_ids[i]] = [chapter_ids[i + 1]]
        return graph

    @classmethod
    def _synthesize_story_events(cls, chapter: Dict[str, Any]) -> None:
        """ä» completion_conditions æœºæ¢°ç”Ÿæˆæœ€å°å¯ç”¨ v2 eventsã€‚"""
        chapter_id = str(chapter.get("id") or "unknown").strip() or "unknown"

        completion = chapter.get("completion_conditions")
        if not isinstance(completion, dict):
            completion = {}
            chapter["completion_conditions"] = completion

        required_raw = completion.get("events_required")
        required_events: List[str] = []
        if isinstance(required_raw, list):
            for event_id in required_raw:
                if isinstance(event_id, str) and event_id.strip():
                    required_events.append(event_id.strip())
        if not required_events:
            required_events = [f"{chapter_id}_event_1"]

        synthesized_events: List[Dict[str, Any]] = []
        prev_event_id: Optional[str] = None
        for event_id in required_events:
            conditions = []
            if prev_event_id:
                conditions.append({
                    "type": "event_triggered",
                    "params": {"event_id": prev_event_id},
                })
            else:
                # ç¬¬ä¸€äº‹ä»¶è®¾ç½®ä¸ºå›åˆ0å³å¯è§¦å‘ï¼Œä¿è¯ strict-v2 ä¸‹ç« èŠ‚å¯æ¨è¿›
                conditions.append({
                    "type": "rounds_elapsed",
                    "params": {"min_rounds": 0},
                })

            synthesized_events.append({
                "id": event_id,
                "name": event_id,
                "description": "Auto-synthesized v2 event from legacy completion_conditions",
                "is_required": True,
                "is_repeatable": False,
                "cooldown_rounds": 0,
                "trigger_conditions": {
                    "operator": "and",
                    "conditions": conditions,
                },
                "narrative_directive": "",
                "side_effects": [],
            })
            prev_event_id = event_id

        chapter["events"] = synthesized_events
        completion["events_required"] = required_events

        tags_raw = chapter.get("tags")
        tags = tags_raw if isinstance(tags_raw, list) else []
        if "auto_migrated_v2" not in tags:
            tags.append("auto_migrated_v2")
        chapter["tags"] = tags

    @classmethod
    def _ensure_v2_story_defaults(cls, mainlines_data: Dict[str, Any]) -> None:
        """è¡¥é½ strict-v2 éœ€è¦çš„ç« èŠ‚å’Œä¸»çº¿å­—æ®µã€‚"""
        chapters = mainlines_data.get("chapters")
        if not isinstance(chapters, list):
            return

        valid_chapter_ids: List[str] = []
        chapters_by_mainline: Dict[str, List[str]] = {}
        for chapter in chapters:
            if not isinstance(chapter, dict):
                continue

            chapter_id = str(chapter.get("id") or "").strip()
            if chapter_id:
                valid_chapter_ids.append(chapter_id)

            mainline_id = str(chapter.get("mainline_id") or "").strip()
            if mainline_id and chapter_id:
                chapters_by_mainline.setdefault(mainline_id, []).append(chapter_id)

            chapter_type = str(chapter.get("type") or "story").strip().lower()
            if chapter_type != "story":
                continue

            events_raw = chapter.get("events")
            if not isinstance(events_raw, list) or not events_raw:
                cls._synthesize_story_events(chapter)

            if not isinstance(chapter.get("transitions"), list):
                chapter["transitions"] = []
            if not isinstance(chapter.get("pacing"), dict):
                chapter["pacing"] = {
                    "min_rounds": 3,
                    "ideal_rounds": 10,
                    "max_rounds": 30,
                    "stall_threshold": 5,
                    "hint_escalation": [
                        "subtle_environmental",
                        "npc_reminder",
                        "direct_prompt",
                        "forced_event",
                    ],
                }
            if "entry_conditions" not in chapter:
                chapter["entry_conditions"] = None
            if not isinstance(chapter.get("tags"), list):
                chapter["tags"] = []

            # è‹¥ completion_conditions ç¼ºå¤± events_requiredï¼Œå›å¡« required event ids
            completion = chapter.get("completion_conditions")
            if not isinstance(completion, dict):
                completion = {}
                chapter["completion_conditions"] = completion
            required = completion.get("events_required")
            if not isinstance(required, list) or not required:
                required_ids: List[str] = []
                for ev in chapter.get("events", []):
                    if not isinstance(ev, dict):
                        continue
                    event_id = str(ev.get("id") or "").strip()
                    if not event_id:
                        continue
                    if ev.get("is_required", True):
                        required_ids.append(event_id)
                if not required_ids and chapter.get("events"):
                    first = chapter["events"][0]
                    if isinstance(first, dict):
                        first_id = str(first.get("id") or "").strip()
                        if first_id:
                            required_ids = [first_id]
                if required_ids:
                    completion["events_required"] = required_ids

        valid_set = set(valid_chapter_ids)
        mainlines = mainlines_data.get("mainlines")
        if not isinstance(mainlines, list):
            return

        for mainline in mainlines:
            if not isinstance(mainline, dict):
                continue
            mainline_id = str(mainline.get("id") or "").strip()

            chapters_raw = mainline.get("chapters")
            normalized_chapters: List[str] = []
            if isinstance(chapters_raw, list):
                for cid in chapters_raw:
                    if isinstance(cid, str) and cid.strip() and cid.strip() in valid_set:
                        normalized_chapters.append(cid.strip())
            if not normalized_chapters and mainline_id in chapters_by_mainline:
                normalized_chapters = list(chapters_by_mainline[mainline_id])
            # å»é‡ä¿åº
            normalized_chapters = list(dict.fromkeys(normalized_chapters))
            mainline["chapters"] = normalized_chapters

            # Transitions-first DAG æ„å»ºç­–ç•¥
            # 1. ä»ç« èŠ‚çš„ transitions å­—æ®µæ„å»º DAG
            transition_graph: Dict[str, List[str]] = {}
            chapters_data = mainlines_data.get("chapters", [])
            for chapter in chapters_data:
                if not isinstance(chapter, dict):
                    continue
                ch_id = str(chapter.get("id", "")).strip()
                if ch_id not in valid_set:
                    continue
                transitions = chapter.get("transitions", [])
                if isinstance(transitions, list):
                    targets = []
                    for trans in transitions:
                        if isinstance(trans, dict):
                            target = str(trans.get("target_chapter_id", "")).strip()
                            if target and target in valid_set:
                                targets.append(target)
                    if targets:
                        transition_graph[ch_id] = list(dict.fromkeys(targets))

            # 2. åˆå¹¶å·²æœ‰ chapter_graphï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            graph_raw = mainline.get("chapter_graph")
            if isinstance(graph_raw, dict) and graph_raw:
                for src, targets in graph_raw.items():
                    src_id = str(src).strip()
                    if not src_id or src_id not in valid_set:
                        continue
                    if not isinstance(targets, list):
                        continue
                    valid_targets = [
                        t.strip() for t in targets
                        if isinstance(t, str) and t.strip() in valid_set
                    ]
                    if valid_targets:
                        existing = transition_graph.get(src_id, [])
                        merged = list(dict.fromkeys(existing + valid_targets))
                        transition_graph[src_id] = merged

            # 3. å¯¹ mainline å†…ç¼ºå¤±çš„èŠ‚ç‚¹è¡¥çº¿æ€§è¾¹
            for i, ch_id in enumerate(normalized_chapters[:-1]):
                if ch_id not in transition_graph:
                    transition_graph[ch_id] = [normalized_chapters[i + 1]]

            if transition_graph:
                mainline["chapter_graph"] = transition_graph
            else:
                mainline["chapter_graph"] = cls._build_linear_chapter_graph(normalized_chapters)

    @staticmethod
    def _cn_num_to_int(cn: str) -> int:
        """ç®€æ˜“ä¸­æ–‡æ•°å­—è½¬æ•´æ•°"""
        cn_map = {
            "ä¸€": 1, "äºŒ": 2, "ä¸‰": 3, "å››": 4, "äº”": 5,
            "å…­": 6, "ä¸ƒ": 7, "å…«": 8, "ä¹": 9, "å": 10,
            "ç™¾": 100,
        }
        if len(cn) == 1:
            return cn_map.get(cn, 1)
        # å¤„ç† "åä¸€" ~ "åä¹"
        if cn.startswith("å"):
            return 10 + cn_map.get(cn[1:], 0)
        # å¤„ç† "äºŒå" ~ "ä¹åä¹"
        if "å" in cn:
            parts = cn.split("å")
            tens = cn_map.get(parts[0], 0) * 10
            ones = cn_map.get(parts[1], 0) if parts[1] else 0
            return tens + ones
        return cn_map.get(cn, 1)

    @staticmethod
    def _needs_chapter_orchestration(mainlines_data: Dict[str, Any]) -> bool:
        """åˆ¤æ–­ mainlines.json æ˜¯å¦ç¼ºå¤± v2 ç« èŠ‚ç¼–æ’å­—æ®µã€‚"""
        chapters = mainlines_data.get("chapters")
        if not isinstance(chapters, list):
            return False

        for chapter in chapters:
            if not isinstance(chapter, dict):
                continue
            if chapter.get("type") != "story":
                continue
            has_events = bool(chapter.get("events"))
            has_transitions = bool(chapter.get("transitions"))
            has_pacing = bool(chapter.get("pacing"))
            if not (has_events or has_transitions or has_pacing):
                return True

        return False

    @staticmethod
    def _validate_mainlines_v2(mainlines_data: Dict[str, Any]) -> None:
        """strict-v2 æ ¡éªŒï¼šstory ç« èŠ‚å¿…é¡»æºå¸¦å¯ç”¨ç¼–æ’å­—æ®µã€‚"""
        chapters = mainlines_data.get("chapters")
        if not isinstance(chapters, list):
            raise ValueError("mainlines.json ç¼ºå°‘ chapters åˆ—è¡¨")

        # 5a. chapter_id å”¯ä¸€æ€§æ ¡éªŒ
        all_ids = [ch.get("id") for ch in chapters if isinstance(ch, dict)]
        duplicates = [cid for cid in all_ids if all_ids.count(cid) > 1]
        if duplicates:
            raise ValueError(
                f"strict-v2 æ ¡éªŒå¤±è´¥: å­˜åœ¨é‡å¤ chapter_id: {set(duplicates)}"
            )

        for chapter in chapters:
            if not isinstance(chapter, dict):
                continue
            if chapter.get("type") != "story":
                continue
            chapter_id = chapter.get("id", "unknown")
            if not isinstance(chapter.get("events"), list) or not chapter.get("events"):
                raise ValueError(f"strict-v2 æ ¡éªŒå¤±è´¥: chapter={chapter_id} ç¼ºå°‘æœ‰æ•ˆ events")
            if not isinstance(chapter.get("transitions"), list):
                raise ValueError(f"strict-v2 æ ¡éªŒå¤±è´¥: chapter={chapter_id} transitions éåˆ—è¡¨")
            if not isinstance(chapter.get("pacing"), dict):
                raise ValueError(f"strict-v2 æ ¡éªŒå¤±è´¥: chapter={chapter_id} pacing éå¯¹è±¡")

        # 5b. çœŸå®æå– vs å…œåº•åˆæˆ è´¨é‡é—¨æ§
        story_chapters = [
            ch for ch in chapters
            if isinstance(ch, dict) and ch.get("type") == "story"
        ]
        auto_migrated_count = sum(
            1 for ch in story_chapters
            if "auto_migrated_v2" in (ch.get("tags") or [])
        )
        auto_ratio = auto_migrated_count / max(len(story_chapters), 1)
        if auto_ratio > 0.5:
            raise ValueError(
                f"strict-v2 è´¨é‡æ ¡éªŒå¤±è´¥: {auto_migrated_count}/{len(story_chapters)} "
                f"story ç« èŠ‚ä»ä¸ºå…œåº•åˆæˆ (auto_migrated_v2)ï¼Œ"
                f"æ¯”ä¾‹ {auto_ratio:.0%} > 50%ï¼Œè¯·æ£€æŸ¥ Phase 3 orchestration æ˜¯å¦æˆåŠŸè¿è¡Œ"
            )

        # narrative_directive è¦†ç›–ç‡è­¦å‘Š
        total_events = 0
        events_with_directive = 0
        for ch in story_chapters:
            for ev in (ch.get("events") or []):
                if isinstance(ev, dict):
                    total_events += 1
                    if ev.get("narrative_directive"):
                        events_with_directive += 1
        directive_ratio = events_with_directive / max(total_events, 1)
        if directive_ratio < 0.3:
            print(
                f"  WARNING: narrative_directive è¦†ç›–ç‡ä»… {directive_ratio:.0%} "
                f"({events_with_directive}/{total_events})ï¼Œå™è¿°è´¨é‡å¯èƒ½ä¸è¶³"
            )

        mainlines = mainlines_data.get("mainlines")
        if not isinstance(mainlines, list):
            raise ValueError("mainlines.json ç¼ºå°‘ mainlines åˆ—è¡¨")
        for mainline in mainlines:
            if not isinstance(mainline, dict):
                continue
            mainline_id = mainline.get("id", "unknown")
            if "chapter_graph" not in mainline:
                raise ValueError(f"strict-v2 æ ¡éªŒå¤±è´¥: mainline={mainline_id} ç¼ºå°‘ chapter_graph")
            if not isinstance(mainline.get("chapter_graph"), dict):
                raise ValueError(f"strict-v2 æ ¡éªŒå¤±è´¥: mainline={mainline_id} chapter_graph éå¯¹è±¡")


# ==================== Helper Functions ====================


def generate_world_map(maps_data: MapsData) -> WorldMap:
    """
    ä»åœ°å›¾æ•°æ®ç”Ÿæˆä¸–ç•Œåœ°å›¾ç»“æ„ï¼ˆçº¯è§„åˆ™é€»è¾‘ï¼Œæå–è‡ª WorldbookGraphizerï¼‰

    æ ¹æ®åœ°å›¾çš„ region å­—æ®µè‡ªåŠ¨åˆ†ç»„
    """
    import re

    def _to_id(name: str) -> str:
        cleaned = re.sub(r'[^\w\s]', '', name)
        return cleaned.replace(' ', '_').lower()

    # æŒ‰ region åˆ†ç»„
    region_maps: Dict[str, list] = {}
    for m in maps_data.maps:
        region = m.region or "æœªçŸ¥åŒºåŸŸ"
        if region not in region_maps:
            region_maps[region] = []
        region_maps[region].append(m)

    # ç”ŸæˆåŒºåŸŸ
    danger_levels = {"low": 0, "medium": 1, "high": 2, "extreme": 3}
    reverse_danger = {v: k for k, v in danger_levels.items()}

    regions = []
    for region_name, maps in region_maps.items():
        max_danger = max(danger_levels.get(m.danger_level, 0) for m in maps)
        danger_level = reverse_danger.get(max_danger, "low")

        regions.append(WorldMapRegion(
            id=_to_id(region_name),
            name=region_name,
            description=f"{region_name}ï¼ŒåŒ…å« {len(maps)} ä¸ªåœ°ç‚¹",
            maps=[m.id for m in maps],
            danger_level=danger_level,
        ))

    # æ”¶é›†è·¨åŒºåŸŸè¿æ¥
    region_by_map = {m.id: m.region or "æœªçŸ¥åŒºåŸŸ" for m in maps_data.maps}
    global_connections = []
    for m in maps_data.maps:
        for conn in m.connections:
            source_region = region_by_map.get(m.id)
            target_region = region_by_map.get(conn.target_map_id)
            if source_region and target_region and source_region != target_region:
                global_connections.append({
                    "from": m.id,
                    "to": conn.target_map_id,
                    "from_region": source_region,
                    "to_region": target_region,
                    "type": conn.connection_type,
                })

    return WorldMap(
        name="æ¸¸æˆä¸–ç•Œ",
        description="ä»ä¸–ç•Œä¹¦è‡ªåŠ¨ç”Ÿæˆçš„ä¸–ç•Œåœ°å›¾",
        regions=regions,
        global_connections=global_connections,
    )


def _save_json(path: Path, data: Any, default=None) -> None:
    """ä¿å­˜ JSON æ–‡ä»¶"""
    path.write_text(
        json.dumps(data, ensure_ascii=False, indent=2, default=default),
        encoding="utf-8",
    )
